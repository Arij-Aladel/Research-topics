<details><summary>  2023 </summary>

1. [How to avoid machine learning pitfalls: guide for academic researchers](https://arxiv.org/pdf/2108.02497.pdf)
2. [Should You Mask 15% in Masked Language Modeling?](https://arxiv.org/pdf/2202.08005.pdf) 10 Feb 2023
3. [The NLP Task Effectiveness of Long-Range Transformers](https://arxiv.org/pdf/2202.07856.pdf)  11 Feb 2023
4. [Learning Better Masking for Better Language Model Pre-training](https://arxiv.org/pdf/2208.10806.pdf) 25 May 2023
5. [LONGNET: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/pdf/2307.02486.pdf)  19 Jul 2023
6. [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf) 9 Sep 2023
7. [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/pdf/2104.09864.pdf)  8 Nov 2023

</details>

<details><summary>  2022 </summary>

1. [Learn To Remember: Transformer with Recurrent Memory for Document-Level Machine Translation](https://arxiv.org/pdf/2205.01546.pdf)  3 May 2022
2. [Exploring Neural Models for Query-Focused Summarization](https://arxiv.org/pdf/2112.07637.pdf)   26 Apr 2022
3. [EXT5: TOWARDS EXTREME MULTI-TASK SCALING FOR TRANSFER LEARNING](https://arxiv.org/pdf/2111.10952.pdf)   29 Jan 2022
4. [cosFormer: Rethinking Softmax in Attention](https://arxiv.org/pdf/2202.08791.pdf) 17  Feb 2022
5. [The Efficiency Misnomer](https://arxiv.org/pdf/2110.12894.pdf)  6 Mar 2022  
6. [HIBRIDS: Attention with Hierarchical Biases for Structure-aware Long Document Summarization](https://arxiv.org/pdf/2203.10741.pdf)  21 Mar 2022
7. [On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations](https://arxiv.org/pdf/2203.13928.pdf)  25 March 2022
8. [Position Information in Transformers:An Overview](https://watermark.silverchair.com/coli_a_00445.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAzswggM3BgkqhkiG9w0BBwagggMoMIIDJAIBADCCAx0GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMiF2pW3jmgMl_l_omAgEQgIIC7luPtILVeDT3W-cICJMGu285No_ZhMuCD6cytZDXtmJ9Zs188Vawlndp7-DDl2HpQeIV4ZtOEwLoSouGeRMqeZMbEqWD4yWRqivJWcQ6qtdWUTpNKyjsQtysX8x-wWU1GaNuh8PkKVXy7w4rZunjHkJTk7sSJ06kDwaGW-I8c8-cxf9gUcRhlCUJ-U8aCflPjW1W-wm8bTD9mndtg5vwHbTMpqvuzuoQ7aJBjgxXAJ6GE08RMP1wNGBmRvT_C3LkZnyBPBx1Xc7g0IeTMrVStOzpEdkJFGsZnj2X_8DyLm1mfrrwnNaY9FgRrYy0JMjYTCjIgnvuxuermAhoenRU0cISkaUm5wXxqUx6Qcen8Au1YT3sK-_uBIWrJdArhETEErtgvzkYLgsqyDs9V6wOBeFIFucodAbgZIcRu4CtUqPj82hqG-n9QxRsLVqCx76QIfzt53am20cwjZSpf4aef58Zv-d1XVhf0ON8O0CWZ8kZem5mlibAqaEebX5bzqipMfPL8Qt0BfyYYOvTXWT7ba6r4hl7UN246bAMVhIs1odMpJSnC8jbRk-_CeOawumwVVxYojp4hwjAhAa3wh8WsTGKG2QzlAbboHc5teZwrQqLXPuFM4pgU7IaeycgY5EQv_Qv4rJuByBZpuIekUIJbMgMhhu7ogr8qd9tYw-eEa-qab1KoXJaAktP1NzxExLfx55BJYuYMUy36Cv1kh2gJxfIHqHO4PI2UIBUUqu2WXDZpOVAKgEtuXKvNnxxJiUOx6T2aAHap1uAmDpn-D6OPcnMO_ttF8XHM9MX--F9NUxeVOo6o6gUni_MY_Ox0AGYk2Gg3efgrwjmgAAcvKlMIT9ka4Tu8BN_P5Gi-2LTi3CEdaBAHM1TFU_tr7H4XBTkXR2Zylk1bgS2xidTrUDOTJF1jqHwvNLOyGqADDr0tK_CFrODh74Fvrimy--oxwlvbbgI3NIuGABmw8XhExqmtzlJAzDwVqCgiqkuFx4xOg)  30 Mrch 2022
9. [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/pdf/2112.07916.pdf)  3 May 2022
10. [Semantic Self-Segmentation for Abstractive Summarization of Long Documents in Low-Resource Regimes](https://www.semanticscholar.org/paper/Semantic-Self-Segmentation-for-Abstractive-of-Long-Moro-Ragazzi/4eb45f33446018175e266738be22f4d830ed697e)  28 June 2022
11. [An Empirical Survey on Long Document Summarization:Datasets, Models and Metrics](https://arxiv.org/pdf/2207.00939.pdf)  3 Jul 2022
12. [BLONDE: An Automatic Evaluation Metric for Document-level Machine Translation](https://arxiv.org/pdf/2103.11878.pdf) 5 jul 2022
13. [Scaling Laws vs Model Architectures:How does Inductive Bias Influence Scaling?](https://arxiv.org/pdf/2207.10551.pdf)  21 july 2022
14. [A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models](https://arxiv.org/pdf/2201.05337.pdf) 24 Aug 2023
15. [inearizing Transformer with Key-Value Memory Bank](https://arxiv.org/pdf/2203.12644.pdf)  13 Oct 2022
16. [STAR-Transformer: A Spatio-temporal Cross Attention Transformer for Human Action Recognition](https://arxiv.org/pdf/2210.07503.pdf)  14 Oct 2022
17. [Processing Long Legal Documents with Pre-trained Transformers: Modding LegalBERT and Longformer](https://aclanthology.org/2022.nllp-1.11.pdf)  2 November 2022
18. [Processing Long Legal Documents with Pre-trained Transformers:Modding LegalBERT and Longformer](https://arxiv.org/pdf/2211.00974.pdf) 10 Nov 2022
19. [RETHINKING ATTENTION WITH PERFORMERS](https://arxiv.org/pdf/2009.14794.pdf)   19 Nov 2022
20. [Transformer Language Models without Positional Encodings Still Learn Positional Information](https://arxiv.org/pdf/2203.16634.pdf) 5 Dec 2022
21. [CTRLsum: Towards Generic Controllable Text Summarization](https://aclanthology.org/2022.emnlp-main.396/) December 7-11, 2022
22. [A Length-Extrapolatable Transformer](https://arxiv.org/pdf/2212.10554.pdf)  20 Dec 2022
23. [Efficient Long-Text Understanding with Short-Text Models](https://arxiv.org/pdf/2208.00748.pdf) 27 Dec 2022

    
</details>

<details><summary>  2021 </summary>

1. [Big Bird: Transformers for Longer Sequences](https://arxiv.org/pdf/2007.14062.pdf) 8 Jan 2021
2. [Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering](https://arxiv.org/pdf/2007.01282.pdf)  3 Feb 2021
3. [Efficient Attentions for Long Document Summarization](https://arxiv.org/pdf/2104.02112.pdf)   11 Apr 2021
4. [READTWICE: Reading Very Large Documents with Memories](https://arxiv.org/pdf/2105.04241.pdf)  11 May 2021
5. [Synthesizer: Rethinking Self-Attention for Transformer Models](https://arxiv.org/pdf/2005.00743.pdf) 24 May 2021
6. [Long-Span Summarization via Local Attention and Content Selection](https://arxiv.org/pdf/2105.03801.pdf)   29 May 2021
7. [Controllable Abstractive Dialogue Summarization with Sketch Supervision](https://arxiv.org/abs/2105.14064)  3 Jun 2021
8. [Poolingformer: Long document modeling with pooling attention](https://arxiv.org/pdf/2105.04371.pdf)  24 Oct 2022
4. [Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity](https://arxiv.org/pdf/2101.03961.pdf)  ArXiv  11 January 2021
7. [Hierarchical Learning for Generation with Long Source Sequences](https://arxiv.org/pdf/2104.07545.pdf)  Published 15 April 2021
8. [Long-Span Summarization via Local Attention and Content Selection](https://arxiv.org/pdf/2105.03801.pdf)  8 May 2021
9. [HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization](https://arxiv.org/pdf/1905.06566.pdf) 16 May 2019
10. [G-Transformer for Document-level Machine Translation](https://arxiv.org/pdf/2105.14761.pdf)   31 May 2021
11. [Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long Documents](https://aclanthology.org/2021.naacl-main.470.pdf) June 6–11, 2021
12. [Charformer: Fast character transformers via gradient-based subword tokenization](https://arxiv.org/pdf/2106.12672.pdf)  Published 23 June 2021
13. [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/pdf/2107.14795.pdf)  30 July 2021
14. [Video Paragraph Captioning as a Text Summarization Task](https://aclanthology.org/2021.acl-short.9.pdf)  August 1–6, 2021
15. [CDLM: Cross-Document Language Modeling](https://arxiv.org/pdf/2101.00406.pdf)  2 Sep 2021
16. [Do Transformer Modifications Transfer Across Implementations and Applications?](https://arxiv.org/pdf/2102.11972.pdf)  10 Sep 2021 
17. [SHAPE: Shifted Absolute Position Embedding for Transformers](https://arxiv.org/pdf/2109.05644.pdf)   13 Sep 2021
18. [Context-Adaptive Document-Level Neural Machine Translation](https://arxiv.org/pdf/2104.08259.pdf)  7 Oct 2021
19. [NB-MLM: Efficient Domain Adaptation of Masked Language Models for Sentiment Analysis](https://aclanthology.org/2021.emnlp-main.717.pdf)   November 7–11, 2021
16. [Sparse is Enough in Scaling Transformers](https://arxiv.org/pdf/2111.12763.pdf)  24 Nov 2021
17. [Memory transformer with hierarchical attention for long document processing](https://ieeexplore.ieee.org/document/9681776)  25 November 2021
18. [ GLaM: Efficient scaling of language models with mixtureof-experts. ](https://arxiv.org/pdf/2112.06905.pdf)   13 December 2021
   
    
</details> 
    

<details><summary>  2020 </summary>
    
1. [Reformer: The Efficient Transformer](https://arxiv.org/pdf/2001.04451.pdf)  Published 13 January 2020 , publishe on arive 18 Feb 2020
2. [SpanBERT: Improving Pre-training by Representing and Predicting Spans](https://arxiv.org/pdf/1907.10529.pdf)  18 Jan 2020
3. [Sparse sinkhorn attention](https://arxiv.org/pdf/2002.11296.pdf)    26 February 2020
4. [Efficient Content-Based Sparse Attention with Routing Transformers](https://arxiv.org/pdf/2003.05997.pdf)  12 March 2020
5. [Learning to Encode Position for Transformer with Continuous Dynamical Mode](https://arxiv.org/pdf/2003.09229.pdf)  13 Mar 2020
6. [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/pdf/1907.12461.pdf) 16 April 2020
7. [ETC: Encoding Long and Structured Inputs in Transformers](https://aclanthology.org/2020.emnlp-main.19.pdf)  17 April 2020
8. [From Standard Summarization to New Tasks and Beyond: Summarization with Manifold Information](https://arxiv.org/pdf/2005.04684.pdf) 10 May 2020
9. [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/pdf/1906.08237.pdf)  2 Jan 2020 
10. [SpanBERT: Improving Pre-training by Representing and Predicting Spans](https://arxiv.org/pdf/1907.10529.pdf)  8 Jan 2020
11. [Funnel-transformer: Filtering out sequential redundancy for efficient language processing](https://arxiv.org/pdf/2006.03236.pdf) Published  5 June 2020
12. [GMAT: Global Memory Augmentation for Transformers](https://arxiv.org/pdf/2006.03274.pdf)  5 Jun 2020
13. [Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers](https://arxiv.org/pdf/2006.03555.pdf)  Published 5 June 2020
14. [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/pdf/2006.04768.pdf)  14 Jun 2020
15. [SEAL: Segment-wise Extractive-Abstractive Long-form Text Summarization](https://arxiv.org/pdf/2006.10213.pdf)  18 Jun 2020
16. [Transformers are RNNs: Fast autoregressive transformers with linear attention.](https://arxiv.org/pdf/2006.16236.pdf)    29 June 2020
17. [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/pdf/2006.16668.pdf)  30 June 2020
18. [Do Transformers Need Deep Long-Range Memory?](https://arxiv.org/pdf/2007.03356.pdf)  7 July 20207 July 2020
19. [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/pdf/1912.08777.pdf) 10 Jul 2020
20. [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/pdf/2006.16236.pdf) 31 Aug 2020
21. [A Divide-and-Conquer Approach to the Summarization of Long Documents](https://arxiv.org/pdf/2004.06190.pdf)    23 Sep 2020  
22. [RETHINKING ATTENTION WITH PERFORMERS](https://arxiv.org/pdf/2009.14794.pdf) 30 sep_2020
23. [Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers](https://arxiv.org/pdf/2006.03555.pdf)   1 Oct 2020
24. [What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding](https://arxiv.org/pdf/2010.04903.pdf)   10 Oct 2020
25. [Rethinking Document-level Neural Machine Translation](https://arxiv.org/pdf/2010.08961.pdf) 18 October 2020
26. [Blockwise Self-Attention for Long Document Understanding](https://arxiv.org/pdf/1911.02972.pdf)  1 Nov 2020
27. [LONG RANGE ARENA: A BENCHMARK FOR EFFICIENT TRANSFORMERS](https://arxiv.org/pdf/2011.04006.pdf)   8 Nov 2020
28. [ETC: Encoding Long and Structured Inputs in Transformers](https://aclanthology.org/2020.emnlp-main.19.pdf)  November 16–20, 2020
29. [Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf)  2 Dec 2020
30. [CTRLSUM: TOWARDS GENERIC CONTROLLABLE TEXT SUMMARIZATION](https://arxiv.org/pdf/2012.04281.pdf)   8 Dec 2020

    
</details>  

<details><summary>  2019 </summary>

1. [Analysis of Positional Encodings for Neural Machine Translation](https://www-i6.informatik.rwth-aachen.de/publications/download/1132/RosendahlJanTranVietAnhKhoaWangWeiyueNeyHermann--AnalysisofPositionalEncodingsforNeuralMachineTranslation--2019.pdf)   2019
2. [Language Models are Unsupervised Multitask Learners](https://gwern.net/doc/ai/nn/transformer/gpt/2019-radford.pdf)  2019
3. [CC-News-En: A Large English News Corpus](https://people.eng.unimelb.edu.au/ammoffat/abstracts/cikm20ccnews.pdf)  2019
4. [Representation Learning with Contrastive Predictive Coding](https://arxiv.org/pdf/1807.03748.pdf)     22 Jan 2019
5. [Cloze-driven Pretraining of Self-attention Networks](https://arxiv.org/pdf/1903.07785.pdf)     19 Mar 2019
6. [Generating long sequences with sparse transformers](https://arxiv.org/pdf/1904.10509.pdf)  Published 23 April 2019
7. [HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization](https://arxiv.org/pdf/1905.06566.pdf)   16 May 2019 
8. [Sample Efficient Text Summarization Using a Single Pre-Trained Transformer](https://arxiv.org/pdf/1905.08836.pdf)  21 May 2019
9. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)  24 May 2019 
10. [Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks](https://arxiv.org/pdf/1810.00825.pdf)  26 May 2019
11. [Transformer-XL: Attentive Language Models beyond a Fixed-Length Context](https://arxiv.org/pdf/1901.02860.pdf)  2 Jun 2019
12. [ERNIE: Enhanced Language Representation with Informative Entities](https://arxiv.org/pdf/1905.07129.pdf)   4 Jun 2019
13. [Large memory layers with product keys](https://arxiv.org/pdf/1907.05242.pdf)   10 July 2019
14. [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf)  26 Jul 2019
15. [Natural Questions: A Benchmark for Question Answering Research](https://aclanthology.org/Q19-1026.pdf)    1 August 2019
16. [Adaptive Attention Span in Transformers](https://arxiv.org/pdf/1905.07799.pdf)   8 Aug 2019
17. [Neural Text Summarization: A Critical Evaluation](https://arxiv.org/pdf/1908.08960.pdf)   23 Aug 2019
18. [Text Summarization with Pretrained Encoders](https://arxiv.org/pdf/1908.08345.pdf)  5 Sep 2019
19. [Generating Logical Forms from Graph Representations of Text and Entities](https://arxiv.org/pdf/1905.08407.pdf)  25 Sep 2019
20. [A Simple Method for Commonsense Reasoning](https://arxiv.org/pdf/1806.02847.pdf)    26 Sep 2019
21. [Evaluating the Factual Consistency of Abstractive Text Summarization](https://arxiv.org/pdf/1910.12840.pdf)       28 October 2019
22. [Text Summarization with Pretrained Encoders.](https://arxiv.org/pdf/1910.12840.pdf)   Published 28 October 2019
23. [Evaluating the Factual Consistency of Abstractive Text Summarization](https://arxiv.org/pdf/1910.12840.pdf)   28 Oct 2019
24. [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf)   29 Oct 2019
25. [SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization](https://aclanthology.org/D19-5409.pdf)  4 Nov 2019
26. [Open Domain Web Keyphrase Extraction Beyond Language Modeling](https://arxiv.org/pdf/1911.02671.pdf)  6 Nov 2019
27. [COMPRESSIVE TRANSFORMERS FOR LONG-RANGE SEQUENCE MODELLING](https://arxiv.org/pdf/1911.05507.pdf)   13 Nov 2019  
    
</details>


<details><summary>  2018 </summary>

1. [Self-Attention with Relative Position Representations](https://arxiv.org/pdf/1803.02155.pdf)  12 Apr 2018
2. [Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks](https://arxiv.org/pdf/1810.00825.pdf)   26 May 2019
3. [Generating Wikipedia by summarizing long sequences](https://arxiv.org/pdf/1801.10198.pdf)  30 Jan 2018
4. [NEWSROOM: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies](https://aclanthology.org/N18-1065.pdf)   June 1 - 6, 2018
5. [Constructing Datasets for Multi-hop Reading Comprehension Across Documents](https://arxiv.org/pdf/1710.06481.pdf)   11 Jun 2018
6. [HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering](https://arxiv.org/pdf/1809.09600.pdf)  25 Sep 2018
7. [Uncovering divergent linguistic information in word embeddings with lessons for intrinsic and extrinsic evaluation](https://aclanthology.org/K18-1028.pdf) October 31 - November 1, 2018
8. [MUSIC TRANSFORMER: GENERATING MUSIC WITH LONG-TERM STRUCTURE](https://arxiv.org/pdf/1809.04281.pdf)  12 Dec 2018 
9. 

    
</details> 

<details><summary>  2017 </summary>

1. [Get To The Point: Summarization with Pointer-Generator Networks](https://aclanthology.org/P17-1099.pdf)   Published 1 April 2017
2. [Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/pdf/1704.04368.pdf)  25 Apr 2017
3. [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)     12 June 2017
4. [Convolutional Sequence to Sequence Learning](https://arxiv.org/pdf/1705.03122.pdf)     25 Jul 2017
5. [DECOUPLED WEIGHT DECAY REGULARIZATION](https://openreview.net/pdf?id=Bkg6RiCqY7)   14 November 2017
    
</details> 

<details><summary>  2016 </summary>

1. [Generating Sentences from a Continuous Space](https://arxiv.org/pdf/1511.06349.pdf)    12 May 2016
2. [Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints](https://arxiv.org/pdf/1603.08887.pdf)   8 Jun 2016
3. [SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://arxiv.org/pdf/1606.05250v1.pdf)  16 Jun 2016
4. []()
    
</details> 

<details><summary>  2015 </summary>

1. [A Neural Attention Model for Abstractive Sentence Summarization](https://arxiv.org/pdf/1509.00685.pdf)  3 Sep 2015
2. [Teaching Machines to Read and Comprehend](https://arxiv.org/pdf/1506.03340.pdf)   19 Nov 2015

    
</details> 

<details><summary>  2014 </summary>

6. 
    
</details> 

<details><summary>  2013 </summary>

1.[Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](https://aclanthology.org/P11-1015.pdf)  19 June 2011 
    
</details> 

<details><summary>  2011 </summary>

1.[Learning Word Vectors for Sentiment Analysis](https://aclanthology.org/P11-1015.pdf)   19 June 2011 
    
</details> 

<details><summary>  2008 </summary>

1.[A unified architecture for natural language processing: deep neural networks with multitask learning](http://machinelearning.org/archive/icml2008/papers/391.pdf)   5 July 2008
</details> 


<details><summary>  To read </summary>

1. [Efficient Long-Text Understanding with Short-Text Models](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00547/115346/Efficient-Long-Text-Understanding-with-Short-Text)
2. [Simple Local Attentions Remain Competitive for Long-Context Tasks](https://arxiv.org/pdf/2112.07210.pdf) 4 May 2022
3. [Adapting Pretrained Text-to-Text Models for Long Text Sequences](https://arxiv.org/pdf/2209.10052.pdf)  16 Nov 2022
4. [Investigating Efficiently Extending Transformers Long Input Summarization](https://arxiv.org/pdf/2208.04347.pdf)  8 Aug 2022
5. [A Survey on Long Text Modeling with Transformers](https://arxiv.org/pdf/2302.14502.pdf)  28 Feb 2023
6. [How Far are We from Robust Long Abstractive Summarization?](https://arxiv.org/pdf/2210.16732.pdf)  30 Oct 2022
7. [ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding](https://arxiv.org/pdf/2305.14196.pdf) 23 May 2023
8. [In-context Autoencoder for Context Compression in a Large Language Model](https://arxiv.org/pdf/2307.06945.pdf)  13 Jul 2023
9. [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/pdf/2307.03172.pdf) 6 Jul 2023
10. [Position Information in Transformers:An Overview](https://arxiv.org/pdf/2102.11090.pdf)  9 Sep 2021
11. [mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences](https://arxiv.org/pdf/2305.11129.pdf)  18 May 2023 
13. [Dynamic Masking Rate Schedules for MLM Pretraining](https://arxiv.org/pdf/2305.15096.pdf)
14. [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf) 26 Jul 2019
15. [Cross-Attention is All You Need:Adapting Pretrained Transformers for Machine Translation](https://aclanthology.org/2021.emnlp-main.132.pdf)
16. [Efficient Transformers: A Survey](https://arxiv.org/pdf/2009.06732.pdf)   4 Mar 2022
17. [PONET: POOLING NETWORK FOR EFFICIENT TOKEN MIXING IN LONG SEQUENCES](https://arxiv.org/pdf/2110.02442.pdf) 22 May 2023
18. [DEBERTAV3: IMPROVING DEBERTA USING ELECTRA-STYLE PRE-TRAINING WITH GRADIENTDISENTANGLED EMBEDDING SHARING](https://arxiv.org/pdf/2111.09543.pdf) 24 Mar 2023
19. [COLT5: Faster Long-Range Transformers with Conditional Computation](https://arxiv.org/pdf/2303.09752.pdf) 14 Apr 2023
20. [AWESOME: GPU Memory-constrained Long Document Summarization using Memory Mechanism and Global Salient Content](https://arxiv.org/pdf/2305.14806.pdf) 24 May 2023
21. [Adapting Language Models to Compress Contexts](https://arxiv.org/pdf/2305.14788.pdf) 24 May 2023
22. [Long-range Language Modeling with Self-retrieval](https://arxiv.org/pdf/2306.13421.pdf)  23 Jun 2023
23. [LONG RANGE ARENA: A BENCHMARK FOR EFFICIENTTRANSFORMERS](https://arxiv.org/pdf/2011.04006.pdf)  8 Nov 2020
24. [Block-State Transformer](https://arxiv.org/pdf/2306.09539.pdf)  15 Jun 2023
25. [Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?](https://arxiv.org/pdf/2207.10551.pdf)  21 Jul 2022
26. [Emergent Abilities of Large Language Models](https://arxiv.org/pdf/2206.07682.pdf)   26 Oct 2022
27. [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://arxiv.org/pdf/2004.12832.pdf) 4 Jun 2020
28. [An Experimental Study on Pretraining Transformers from Scratch for IR](https://arxiv.org/pdf/2301.10444.pdf)   25 Jan 2023
29. [In-context Autoencoder for Context Compression in a Large Language Model](https://arxiv.org/pdf/2307.06945.pdf) 13 Jul 2023
30. [Adapting Language Models to Compress Contexts](https://arxiv.org/pdf/2305.14788.pdf#cite.RMT)   24 May 2023
31. [Blockwise Compression of Transformer-based Models without Retraining](https://arxiv.org/pdf/2304.01483.pdf)  4 Apr 2023
32. [Hypoformer: Hybrid Decomposition Transformer for Edge-friendly Neural Machine Translation](https://aclanthology.org/2022.emnlp-main.475.pdf)
33. [Text Compression-aided Transformer Encoding](https://arxiv.org/pdf/2102.05951.pdf)   11 Feb 2021
34. [GROUPED SELF-ATTENTION MECHANISM FOR A MEMORY-EFFICIENT TRANSFORMER](https://arxiv.org/pdf/2210.00440.pdf) 6 Oct 2022
35. [Shortformer: Better Language Modeling Using Shorter Inputs](https://aclanthology.org/2021.acl-long.427.pdf)
36. [Shortformer: Better Language Modeling Using Shorter Inputs](https://aclanthology.org/2021.acl-long.427.pdf)  August 1–6, 2021. Facebook AI Research, 3Allen Institute for AI
37. [A Length-Extrapolatable Transformer](https://arxiv.org/pdf/2212.10554.pdf)    20 Dec 2022
38. [he Stack: 3 TB of permissively licensed source code](https://arxiv.org/pdf/2211.15533.pdf)        20 Nov 2022
39. [https://arxiv.org/pdf/2110.08207.pdf](MULTITASK PROMPTED TRAINING ENABLES ZERO-SHOT TASK GENERALIZATION)   17 March 2022
    
    
</details> 
