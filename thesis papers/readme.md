<details><summary>  2023 </summary>

1. [How to avoid machine learning pitfalls: guide for academic researchers](https://arxiv.org/pdf/2108.02497.pdf)

</details>

<details><summary>  2022 </summary>

1. [Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?](https://arxiv.org/pdf/2207.10551.pdf)
2. [CTRLsum: Towards Generic Controllable Text Summarization](https://aclanthology.org/2022.emnlp-main.396/)
3. [HIBRIDS: Attention with Hierarchical Biases for Structure-aware Long Document Summarization](https://arxiv.org/pdf/2203.10741.pdf)
4. [Semantic Self-Segmentation for Abstractive Summarization of Long Documents in Low-Resource Regimes](https://www.semanticscholar.org/paper/Semantic-Self-Segmentation-for-Abstractive-of-Long-Moro-Ragazzi/4eb45f33446018175e266738be22f4d830ed697e)  28 June 2022

    
</details>

<details><summary>  2021 </summary>

4. [Controllable Abstractive Dialogue Summarization with Sketch Supervision](https://arxiv.org/abs/2105.14064)
5. [Poolingformer: Long document modeling with pooling attention](https://arxiv.org/pdf/2105.04371.pdf)
1. [Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity](https://arxiv.org/pdf/2101.03961.pdf)  ArXiv  11 January 2021
1. [Hierarchical Learning for Generation with Long Source Sequences](https://arxiv.org/pdf/2104.07545.pdf)  Published 15 April 2021
1. [Long-Span Summarization via Local Attention and Content Selection](https://arxiv.org/pdf/2105.03801.pdf)  8 May 2021
2. [HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization](https://arxiv.org/pdf/1905.06566.pdf)
1. [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/pdf/2112.07916.pdf)
1. [Charformer: Fast character transformers via gradient-based subword tokenization](https://arxiv.org/pdf/2106.12672.pdf)  Published 23 June 2021
1. [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/pdf/2107.14795.pdf)  30 July 2021
1. [ GLaM: Efficient scaling of language models with mixtureof-experts. ](https://arxiv.org/pdf/2112.06905.pdf)   13 December 2021
    
</details> 
    

<details><summary>  2020 </summary>
    
5. [Reformer: The Efficient Transformer](https://arxiv.org/pdf/2001.04451.pdf)  Published 13 January 2020
5. [Sparse sinkhorn attention](https://arxiv.org/pdf/2002.11296.pdf)    26 February 2020
5. [Efficient Content-Based Sparse Attention with Routing Transformers](https://arxiv.org/pdf/2003.05997.pdf)  12 March 2020
5. [A divide-and-conquer approach to the summarization of long documents.](https://arxiv.org/pdf/2004.06190.pdf)
1. [ETC: Encoding Long and Structured Inputs in Transformers](https://aclanthology.org/2020.emnlp-main.19.pdf)  17 April 2020
1. [Funnel-transformer: Filtering out sequential redundancy for efficient language processing](https://arxiv.org/pdf/2006.03236.pdf) Published  5 June 2020
6. [Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers](https://arxiv.org/pdf/2006.03555.pdf)  Published 5 June 2020
1. [Transformers are RNNs: Fast autoregressive transformers with linear attention.](https://arxiv.org/pdf/2006.16236.pdf)    29 June 2020
1. [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/pdf/2006.16668.pdf)  30 June 2020
1. [Do Transformers Need Deep Long-Range Memory?](https://arxiv.org/pdf/2007.03356.pdf)  7 July 20207 July 2020
3. [A Divide-and-Conquer Approach to the Summarization of Long Documents](https://arxiv.org/pdf/2004.06190.pdf) 3 Sep 2020
4. [RETHINKING ATTENTION WITH PERFORMERS](https://arxiv.org/pdf/2009.14794.pdf) 30 sep_2020

    
</details>  

<details><summary>  2019 </summary>

1. [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/pdf/2006.16236.pdf)
2. [Generating long sequences with sparse transformers](https://arxiv.org/pdf/1904.10509.pdf)  Published 23 April 2019
3. [Large memory layers with product keys](https://arxiv.org/pdf/1907.05242.pdf)   10 July 2019
4. [Evaluating the Factual Consistency of Abstractive Text Summarization](https://arxiv.org/pdf/1910.12840.pdf)       28 October 2019
5. [Text Summarization with Pretrained Encoders.](https://arxiv.org/pdf/1910.12840.pdf)   Published 28 October 2019
    
</details>


<details><summary>  2018 </summary>

1. [Generating Wikipedia by summarizing long sequences](https://arxiv.org/pdf/1801.10198.pdfhttps://arxiv.org/pdf/1801.10198.pdf)
2. [Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks](https://arxiv.org/pdf/1810.00825.pdf)

    
</details> 

<details><summary>  2017 </summary>

1. [Get To The Point: Summarization with Pointer-Generator Networks](https://aclanthology.org/P17-1099.pdf)   Published 1 April 2017
    
</details> 

<details><summary>  2016 </summary>

1. [Learning-based single-document summarization with compression and anaphoricity constraints]()
    
</details> 

<details><summary>  2015 </summary>

1. 
    
</details> 

<details><summary>  2014 </summary>

1. 
    
</details> 

<details><summary>  2013 </summary>

1. 
    
</details> 
